{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "\n",
    "log_dir = f'./logs/hyperparameter_tuning_BaseModel/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from KAGGLE_NLP_with_disaster_tweets.model import utils, constants\n",
    "from KAGGLE_NLP_with_disaster_tweets.data_preparation.utils import DataPipeline\n",
    "\n",
    "from tensorboard import notebook"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the file: ../data/sample_submission.csv\n",
      "Getting the file: ../data/train.csv\n",
      "Dataframe size before eliminating too short texts: 7613\n",
      "Dataframe size after eliminating too short texts: 7529\n",
      "         id keyword location  \\\n",
      "0         1                    \n",
      "1         4                    \n",
      "2         5                    \n",
      "3         6                    \n",
      "4         7                    \n",
      "...     ...     ...      ...   \n",
      "7608  10869                    \n",
      "7609  10870                    \n",
      "7610  10871                    \n",
      "7611  10872                    \n",
      "7612  10873                    \n",
      "\n",
      "                                                   text  target  \n",
      "0     our deeds are the reason of this earthquake ma...       1  \n",
      "1                 forest fire near la ronge sask canada       1  \n",
      "2     all residents asked to shelter in place are be...       1  \n",
      "3     13  0 people receive wildfires evacuation orde...       1  \n",
      "4     just got sent this photo from ruby alaska as s...       1  \n",
      "...                                                 ...     ...  \n",
      "7608  two giant cranes holding a bridge collapse int...       1  \n",
      "7609  user user the out of control wild fires in cal...       1  \n",
      "7610   m 1  94 0 1  04 utc 5 km s of volcano hawaii url       1  \n",
      "7611  police investigating after an ebike collided w...       1  \n",
      "7612  the latest more homes razed by northern califo...       1  \n",
      "\n",
      "[7529 rows x 5 columns]\n",
      "Vocabulary size of the vectorizer: 17573\n",
      "-----------------------------------------------------------------------------------------\n",
      "Dataset \n",
      "Size: 7529 data points\n",
      "Dataset examples:\n",
      "Input: [ 136 6400   24    3  948    8   21  176  163 1997 6138   81   45    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "Target: [0. 1.]\n",
      "Input: [ 112   28  419  641 9614 9433  479    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "Target: [0. 1.]\n",
      "Input: [   45  1857  1789     7  2069     6   740    24   155 11038    20  1877\n",
      "    43   547   171    68  2069     6   740  1475    24  1175     0     0\n",
      "     0     0     0     0     0     0]\n",
      "Target: [0. 1.]\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_file_name = 'train.csv'\n",
    "kaggle_test_file_name = 'test.csv'\n",
    "sample_submission_file_name = 'sample_submission.csv'\n",
    "\n",
    "max_vocabulary_size = 20000\n",
    "glove_embedding_dim = 200\n",
    "\n",
    "data_pipeline = DataPipeline(train_file_name, kaggle_test_file_name, sample_submission_file_name,\n",
    "                             max_vocabulary_size=max_vocabulary_size,\n",
    "                             output_sequence_length=30,\n",
    "                             glove_embedding_dim=glove_embedding_dim,\n",
    "                             glove_url=constants.glove_url['twitter.27B'])\n",
    "dataset = data_pipeline.prepare_train_dataset(include_cols=[\"keyword\", \"location\"], extract_extras=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "balanced_class_weights = utils.get_balanced_class_weights(data_pipeline.dataframe)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [],
   "source": [
    "# create a dict to keep the hyperparameters\n",
    "\n",
    "hparams = {\n",
    "    hp.HParam(\"optimizer\", hp.Discrete([\"adamw\"])): \"adamw\",\n",
    "    hp.HParam(\"batch_size\", hp.Discrete([16, 32, 64])): 16,\n",
    "    hp.HParam(\"learning_rate\", hp.Discrete([5e-4, 1e-4])): 1e-4,\n",
    "    hp.HParam(\"class_weights\", hp.Discrete([\"none\", \"balanced\"])): \"balanced\",\n",
    "    hp.HParam(\"dropout\", hp.Discrete([0.1, 0.4])): 0.1\n",
    "}\n",
    "# easier to manage hyperparameters with this wrapper class\n",
    "hyperparameter_manager = utils.HyperparameterManager(hparams=hparams)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "session_len = 1\n",
    "for key in list(hparams.keys()):\n",
    "    session_len *= len(key.domain.values)\n",
    "\n",
    "print(session_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [],
   "source": [
    "# remove previous logs\n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "\n",
    "os.makedirs(log_dir)\n",
    "\n",
    "with tf.summary.create_file_writer(log_dir).as_default():\n",
    "    hp.hparams_config(hparams=list(hparams.keys()),\n",
    "                      metrics=constants.metrics)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [],
   "source": [
    "from KAGGLE_NLP_with_disaster_tweets.model.base_model import BaseModel\n",
    "from KAGGLE_NLP_with_disaster_tweets.data_preparation.utils import BatchPipeline\n",
    "\n",
    "\n",
    "def set_model_hparams(data_pipeline, hparam_manager, batch_pipeline, epochs: int, embeddings_initializer):\n",
    "    \"\"\"\n",
    "    Convenience method for initializing the model for hyperparameter optimization\n",
    "    \"\"\"\n",
    "    return BaseModel(\n",
    "            vocabulary_size=data_pipeline.vocabulary_size,\n",
    "            embedding_dim=data_pipeline.glove_embedding_dim,\n",
    "            lstm_dims=[128, 64],\n",
    "            hidden_dim=64,\n",
    "            num_classes=2,\n",
    "            epochs=epochs,\n",
    "            batch_pipeline=batch_pipeline,\n",
    "            hparam_manager=hparam_manager,\n",
    "            embeddings_initializer=embeddings_initializer)\n",
    "\n",
    "\n",
    "def find_best_hparams(dataset: tf.data.Dataset, data_pipeline, hyperparameter_manager, epochs: int,\n",
    "                      balanced_class_weights: dict(), log_directory: str):\n",
    "    \"\"\"\n",
    "    method iteratively looks for all possible hyperparameters in the search domain\n",
    "    :param dataset: tf.data.Dataset\n",
    "    :param data_pipeline: A DataPipeline instance\n",
    "    :param hyperparameter_manager: A HyperparameterManager instance\n",
    "    :param epochs: epochs for hparam optimization\n",
    "    :param balanced_class_weights: dict() of balanced class weights\n",
    "    :param log_directory: where the logs should be saved\n",
    "    \"\"\"\n",
    "    session_num = 1\n",
    "    # initialize embedding initializer once\n",
    "    embeddings_initializer = data_pipeline.build_embeddings_initializer()\n",
    "    # look for the best hyperparameters\n",
    "    for optimizer in hyperparameter_manager.optimizer_hparams.domain.values:\n",
    "        for batch_size in hyperparameter_manager.batch_size_hparams.domain.values:\n",
    "            for learning_rate in hyperparameter_manager.learning_rate_hparams.domain.values:\n",
    "                for dropout in hyperparameter_manager.dropout_hparams.domain.values:\n",
    "                    for class_weights in hyperparameter_manager.class_weights_hparams.domain.values:\n",
    "                        print(f\"\\n********************    Session {session_num}/{session_len} started    **********************\\n\")\n",
    "                        hparams = hyperparameter_manager.set_hparams(optimizer, batch_size, learning_rate,\n",
    "                                                                     class_weights, dropout)\n",
    "                        batch_pipeline = BatchPipeline(dataset, batch_size)\n",
    "                        model = set_model_hparams(data_pipeline, hyperparameter_manager, batch_pipeline, epochs,\n",
    "                                                  embeddings_initializer)\n",
    "\n",
    "                        with tf.summary.create_file_writer(f'{log_directory}{model.run_name}').as_default():\n",
    "                            hp.hparams(hparams)\n",
    "                            accuracy, precision, recall, f1, predictions = model.fit_and_evaluate(\n",
    "                                class_weights=balanced_class_weights,\n",
    "                                log_directory=log_directory)\n",
    "                            tf.summary.scalar(\"accuracy\", accuracy, step=1)\n",
    "                            tf.summary.scalar(\"precision\", precision, step=1)\n",
    "                            tf.summary.scalar(\"recall\", recall, step=1)\n",
    "                            tf.summary.scalar(\"f1\", f1, step=1)\n",
    "                        print(f\"\\n*********************    Session {session_num}/{session_len} ended    ***********************\\n\")\n",
    "                        session_num += 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the file: glove.twitter.27B.200d.txt \n",
      "\n",
      "Found 1193514 word vectors \n",
      "\n",
      "Converted 12866 words, and missed 4707 words.\n",
      "\n",
      "********************    Session 1/24 started    **********************\n",
      "\n",
      "Values are set: Optimizer: adamw | Learning Rate: 0.0001 | Batch Size: 16 | Class Weights: balanced | Dropout Rate: 0.1\n",
      "Model: \"sequential_119\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_119 (Embedding)   (None, None, 200)         3514600   \n",
      "                                                                 \n",
      " dropout_238 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " bidirectional_333 (Bidirect  (None, None, 256)        336896    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_334 (Bidirect  (None, None, 128)        164352    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_238 (Dense)           (None, None, 64)          8256      \n",
      "                                                                 \n",
      " global_average_pooling1d_11  (None, 64)               0         \n",
      " 9 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dropout_239 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_239 (Dense)           (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,024,234\n",
      "Trainable params: 509,634\n",
      "Non-trainable params: 3,514,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "basemodel__lr=0.0001__batch_size=16__optimizer=adamw__class_weights=balanced__dropout=0.1 starting...\n",
      "Epoch 1/10\n",
      "282/282 [==============================] - 18s 44ms/step - loss: 0.6448 - accuracy: 0.6124 - val_loss: 0.5023 - val_accuracy: 0.7626\n",
      "Epoch 2/10\n",
      "282/282 [==============================] - 11s 37ms/step - loss: 0.4633 - accuracy: 0.7883 - val_loss: 0.4532 - val_accuracy: 0.7979\n",
      "Epoch 3/10\n",
      "282/282 [==============================] - 11s 40ms/step - loss: 0.4318 - accuracy: 0.8116 - val_loss: 0.4521 - val_accuracy: 0.7972\n",
      "Epoch 4/10\n",
      "282/282 [==============================] - 11s 40ms/step - loss: 0.4199 - accuracy: 0.8121 - val_loss: 0.4567 - val_accuracy: 0.7999\n",
      "Epoch 5/10\n",
      "282/282 [==============================] - 12s 42ms/step - loss: 0.4081 - accuracy: 0.8218 - val_loss: 0.4489 - val_accuracy: 0.8019\n",
      "Epoch 6/10\n",
      "282/282 [==============================] - 12s 41ms/step - loss: 0.4051 - accuracy: 0.8282 - val_loss: 0.4517 - val_accuracy: 0.8005\n",
      "Epoch 7/10\n",
      "282/282 [==============================] - 12s 42ms/step - loss: 0.4009 - accuracy: 0.8296 - val_loss: 0.4473 - val_accuracy: 0.8045\n",
      "Epoch 8/10\n",
      "282/282 [==============================] - 12s 42ms/step - loss: 0.3896 - accuracy: 0.8340 - val_loss: 0.4483 - val_accuracy: 0.8032\n",
      "Epoch 9/10\n",
      "282/282 [==============================] - 12s 42ms/step - loss: 0.3902 - accuracy: 0.8384 - val_loss: 0.4455 - val_accuracy: 0.8012\n",
      "Epoch 10/10\n",
      "282/282 [==============================] - 12s 43ms/step - loss: 0.3846 - accuracy: 0.8347 - val_loss: 0.4449 - val_accuracy: 0.7999\n",
      "94/94 [==============================] - 2s 12ms/step - loss: 0.4291 - accuracy: 0.8138\n",
      "basemodel__lr=0.0001__batch_size=16__optimizer=adamw__class_weights=balanced__dropout=0.1 completed.\n",
      "\n",
      "*********************    Session 1/24 ended    ***********************\n",
      "\n",
      "\n",
      "********************    Session 2/24 started    **********************\n",
      "\n",
      "Values are set: Optimizer: adamw | Learning Rate: 0.0001 | Batch Size: 16 | Class Weights: none | Dropout Rate: 0.1\n",
      "Model: \"sequential_120\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_120 (Embedding)   (None, None, 200)         3514600   \n",
      "                                                                 \n",
      " dropout_240 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " bidirectional_335 (Bidirect  (None, None, 256)        336896    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_336 (Bidirect  (None, None, 128)        164352    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_240 (Dense)           (None, None, 64)          8256      \n",
      "                                                                 \n",
      " global_average_pooling1d_12  (None, 64)               0         \n",
      " 0 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dropout_241 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_241 (Dense)           (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,024,234\n",
      "Trainable params: 509,634\n",
      "Non-trainable params: 3,514,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "basemodel__lr=0.0001__batch_size=16__optimizer=adamw__class_weights=none__dropout=0.1 starting...\n",
      "Epoch 1/10\n",
      "282/282 [==============================] - 20s 51ms/step - loss: 0.6259 - accuracy: 0.6580 - val_loss: 0.5179 - val_accuracy: 0.7500\n",
      "Epoch 2/10\n",
      "282/282 [==============================] - 12s 43ms/step - loss: 0.4680 - accuracy: 0.7928 - val_loss: 0.4780 - val_accuracy: 0.7799\n",
      "Epoch 3/10\n",
      "282/282 [==============================] - 12s 44ms/step - loss: 0.4372 - accuracy: 0.8078 - val_loss: 0.4686 - val_accuracy: 0.7912\n",
      "Epoch 4/10\n",
      "282/282 [==============================] - 13s 44ms/step - loss: 0.4271 - accuracy: 0.8167 - val_loss: 0.4629 - val_accuracy: 0.7952\n",
      "Epoch 5/10\n",
      "282/282 [==============================] - 13s 47ms/step - loss: 0.4172 - accuracy: 0.8216 - val_loss: 0.4521 - val_accuracy: 0.7972\n",
      "Epoch 6/10\n",
      "282/282 [==============================] - 13s 46ms/step - loss: 0.4055 - accuracy: 0.8287 - val_loss: 0.4494 - val_accuracy: 0.7959\n",
      "Epoch 7/10\n",
      "282/282 [==============================] - 13s 45ms/step - loss: 0.4012 - accuracy: 0.8287 - val_loss: 0.4484 - val_accuracy: 0.7985\n",
      "Epoch 8/10\n",
      "282/282 [==============================] - 13s 45ms/step - loss: 0.3930 - accuracy: 0.8389 - val_loss: 0.4456 - val_accuracy: 0.8045\n",
      "Epoch 9/10\n",
      "282/282 [==============================] - 13s 45ms/step - loss: 0.3866 - accuracy: 0.8380 - val_loss: 0.4457 - val_accuracy: 0.8052\n",
      "Epoch 10/10\n",
      "282/282 [==============================] - 13s 45ms/step - loss: 0.3883 - accuracy: 0.8382 - val_loss: 0.4445 - val_accuracy: 0.8019\n",
      "94/94 [==============================] - 2s 12ms/step - loss: 0.4252 - accuracy: 0.8178\n",
      "basemodel__lr=0.0001__batch_size=16__optimizer=adamw__class_weights=none__dropout=0.1 completed.\n",
      "\n",
      "*********************    Session 2/24 ended    ***********************\n",
      "\n",
      "\n",
      "********************    Session 3/24 started    **********************\n",
      "\n",
      "Values are set: Optimizer: adamw | Learning Rate: 0.0001 | Batch Size: 16 | Class Weights: balanced | Dropout Rate: 0.4\n",
      "Model: \"sequential_121\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_121 (Embedding)   (None, None, 200)         3514600   \n",
      "                                                                 \n",
      " dropout_242 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " bidirectional_337 (Bidirect  (None, None, 256)        336896    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_338 (Bidirect  (None, None, 128)        164352    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_242 (Dense)           (None, None, 64)          8256      \n",
      "                                                                 \n",
      " global_average_pooling1d_12  (None, 64)               0         \n",
      " 1 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dropout_243 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_243 (Dense)           (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,024,234\n",
      "Trainable params: 509,634\n",
      "Non-trainable params: 3,514,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "basemodel__lr=0.0001__batch_size=16__optimizer=adamw__class_weights=balanced__dropout=0.4 starting...\n",
      "Epoch 1/10\n",
      "282/282 [==============================] - 21s 54ms/step - loss: 0.6602 - accuracy: 0.6015 - val_loss: 0.5102 - val_accuracy: 0.7620\n",
      "Epoch 2/10\n",
      "282/282 [==============================] - 13s 46ms/step - loss: 0.4934 - accuracy: 0.7835 - val_loss: 0.4691 - val_accuracy: 0.7872\n",
      "Epoch 3/10\n",
      "282/282 [==============================] - 13s 46ms/step - loss: 0.4662 - accuracy: 0.7948 - val_loss: 0.4619 - val_accuracy: 0.7886\n",
      "Epoch 4/10\n",
      "282/282 [==============================] - 13s 47ms/step - loss: 0.4478 - accuracy: 0.8052 - val_loss: 0.4579 - val_accuracy: 0.7906\n",
      "Epoch 5/10\n",
      "282/282 [==============================] - 13s 47ms/step - loss: 0.4477 - accuracy: 0.8030 - val_loss: 0.4458 - val_accuracy: 0.8019\n",
      "Epoch 6/10\n",
      "282/282 [==============================] - 13s 47ms/step - loss: 0.4354 - accuracy: 0.8081 - val_loss: 0.4475 - val_accuracy: 0.7979\n",
      "Epoch 7/10\n",
      "282/282 [==============================] - 13s 47ms/step - loss: 0.4373 - accuracy: 0.8067 - val_loss: 0.4434 - val_accuracy: 0.7992\n",
      "Epoch 8/10\n",
      "282/282 [==============================] - 13s 47ms/step - loss: 0.4319 - accuracy: 0.8116 - val_loss: 0.4450 - val_accuracy: 0.8012\n",
      "Epoch 9/10\n",
      "282/282 [==============================] - 13s 47ms/step - loss: 0.4272 - accuracy: 0.8107 - val_loss: 0.4453 - val_accuracy: 0.8019\n",
      "Epoch 10/10\n",
      "282/282 [==============================] - 13s 47ms/step - loss: 0.4298 - accuracy: 0.8076 - val_loss: 0.4436 - val_accuracy: 0.7985\n",
      "94/94 [==============================] - 3s 13ms/step - loss: 0.4238 - accuracy: 0.8098\n",
      "basemodel__lr=0.0001__batch_size=16__optimizer=adamw__class_weights=balanced__dropout=0.4 completed.\n",
      "\n",
      "*********************    Session 3/24 ended    ***********************\n",
      "\n",
      "\n",
      "********************    Session 4/24 started    **********************\n",
      "\n",
      "Values are set: Optimizer: adamw | Learning Rate: 0.0001 | Batch Size: 16 | Class Weights: none | Dropout Rate: 0.4\n",
      "Model: \"sequential_122\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_122 (Embedding)   (None, None, 200)         3514600   \n",
      "                                                                 \n",
      " dropout_244 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " bidirectional_339 (Bidirect  (None, None, 256)        336896    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_340 (Bidirect  (None, None, 128)        164352    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_244 (Dense)           (None, None, 64)          8256      \n",
      "                                                                 \n",
      " global_average_pooling1d_12  (None, 64)               0         \n",
      " 2 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dropout_245 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_245 (Dense)           (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,024,234\n",
      "Trainable params: 509,634\n",
      "Non-trainable params: 3,514,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "basemodel__lr=0.0001__batch_size=16__optimizer=adamw__class_weights=none__dropout=0.4 starting...\n",
      "Epoch 1/10\n",
      "282/282 [==============================] - 23s 54ms/step - loss: 0.6462 - accuracy: 0.6398 - val_loss: 0.5549 - val_accuracy: 0.7247\n",
      "Epoch 2/10\n",
      "282/282 [==============================] - 13s 45ms/step - loss: 0.5071 - accuracy: 0.7695 - val_loss: 0.5073 - val_accuracy: 0.7699\n",
      "Epoch 3/10\n",
      "282/282 [==============================] - 13s 47ms/step - loss: 0.4704 - accuracy: 0.7883 - val_loss: 0.4938 - val_accuracy: 0.7826\n",
      "Epoch 4/10\n",
      "282/282 [==============================] - 13s 47ms/step - loss: 0.4539 - accuracy: 0.7999 - val_loss: 0.4844 - val_accuracy: 0.7859\n",
      "Epoch 5/10\n",
      "282/282 [==============================] - 13s 47ms/step - loss: 0.4500 - accuracy: 0.8081 - val_loss: 0.4646 - val_accuracy: 0.7879\n",
      "Epoch 6/10\n",
      "282/282 [==============================] - 13s 47ms/step - loss: 0.4367 - accuracy: 0.8105 - val_loss: 0.4648 - val_accuracy: 0.7926\n",
      "Epoch 7/10\n",
      "282/282 [==============================] - 13s 47ms/step - loss: 0.4393 - accuracy: 0.8147 - val_loss: 0.4514 - val_accuracy: 0.8012\n",
      "Epoch 8/10\n",
      "282/282 [==============================] - 13s 47ms/step - loss: 0.4269 - accuracy: 0.8152 - val_loss: 0.4564 - val_accuracy: 0.7965\n",
      "Epoch 9/10\n",
      "282/282 [==============================] - 13s 47ms/step - loss: 0.4370 - accuracy: 0.8103 - val_loss: 0.4480 - val_accuracy: 0.7965\n",
      "Epoch 10/10\n",
      "282/282 [==============================] - 13s 47ms/step - loss: 0.4266 - accuracy: 0.8183 - val_loss: 0.4492 - val_accuracy: 0.7965\n",
      "94/94 [==============================] - 2s 13ms/step - loss: 0.4176 - accuracy: 0.8105\n",
      "basemodel__lr=0.0001__batch_size=16__optimizer=adamw__class_weights=none__dropout=0.4 completed.\n",
      "\n",
      "*********************    Session 4/24 ended    ***********************\n",
      "\n",
      "\n",
      "********************    Session 5/24 started    **********************\n",
      "\n",
      "Values are set: Optimizer: adamw | Learning Rate: 0.0005 | Batch Size: 16 | Class Weights: balanced | Dropout Rate: 0.1\n",
      "Model: \"sequential_123\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_123 (Embedding)   (None, None, 200)         3514600   \n",
      "                                                                 \n",
      " dropout_246 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " bidirectional_341 (Bidirect  (None, None, 256)        336896    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_342 (Bidirect  (None, None, 128)        164352    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_246 (Dense)           (None, None, 64)          8256      \n",
      "                                                                 \n",
      " global_average_pooling1d_12  (None, 64)               0         \n",
      " 3 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dropout_247 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_247 (Dense)           (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,024,234\n",
      "Trainable params: 509,634\n",
      "Non-trainable params: 3,514,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "basemodel__lr=0.0005__batch_size=16__optimizer=adamw__class_weights=balanced__dropout=0.1 starting...\n",
      "Epoch 1/10\n",
      "282/282 [==============================] - 22s 55ms/step - loss: 0.5602 - accuracy: 0.7119 - val_loss: 0.5668 - val_accuracy: 0.7480\n",
      "Epoch 2/10\n",
      "282/282 [==============================] - 13s 46ms/step - loss: 0.4447 - accuracy: 0.8041 - val_loss: 0.4562 - val_accuracy: 0.7965\n",
      "Epoch 3/10\n",
      "282/282 [==============================] - 14s 49ms/step - loss: 0.4152 - accuracy: 0.8245 - val_loss: 0.4681 - val_accuracy: 0.7959\n",
      "Epoch 4/10\n",
      "282/282 [==============================] - 15s 53ms/step - loss: 0.3941 - accuracy: 0.8362 - val_loss: 0.4606 - val_accuracy: 0.7972\n",
      "Epoch 5/10\n",
      "282/282 [==============================] - 16s 58ms/step - loss: 0.3803 - accuracy: 0.8469 - val_loss: 0.4992 - val_accuracy: 0.7839\n",
      "Epoch 6/10\n",
      "282/282 [==============================] - 17s 61ms/step - loss: 0.3549 - accuracy: 0.8606 - val_loss: 0.5045 - val_accuracy: 0.7846\n",
      "Epoch 7/10\n",
      "282/282 [==============================] - 15s 52ms/step - loss: 0.3427 - accuracy: 0.8648 - val_loss: 0.5459 - val_accuracy: 0.7866\n",
      "Epoch 7: early stopping\n",
      "94/94 [==============================] - 3s 14ms/step - loss: 0.4886 - accuracy: 0.8078\n",
      "basemodel__lr=0.0005__batch_size=16__optimizer=adamw__class_weights=balanced__dropout=0.1 completed.\n",
      "\n",
      "*********************    Session 5/24 ended    ***********************\n",
      "\n",
      "\n",
      "********************    Session 6/24 started    **********************\n",
      "\n",
      "Values are set: Optimizer: adamw | Learning Rate: 0.0005 | Batch Size: 16 | Class Weights: none | Dropout Rate: 0.1\n",
      "Model: \"sequential_124\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_124 (Embedding)   (None, None, 200)         3514600   \n",
      "                                                                 \n",
      " dropout_248 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " bidirectional_343 (Bidirect  (None, None, 256)        336896    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_344 (Bidirect  (None, None, 128)        164352    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_248 (Dense)           (None, None, 64)          8256      \n",
      "                                                                 \n",
      " global_average_pooling1d_12  (None, 64)               0         \n",
      " 4 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dropout_249 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_249 (Dense)           (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,024,234\n",
      "Trainable params: 509,634\n",
      "Non-trainable params: 3,514,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "basemodel__lr=0.0005__batch_size=16__optimizer=adamw__class_weights=none__dropout=0.1 starting...\n",
      "Epoch 1/10\n",
      "282/282 [==============================] - 24s 56ms/step - loss: 0.5554 - accuracy: 0.7196 - val_loss: 0.5631 - val_accuracy: 0.7414\n",
      "Epoch 2/10\n",
      "282/282 [==============================] - 14s 48ms/step - loss: 0.4493 - accuracy: 0.8061 - val_loss: 0.4747 - val_accuracy: 0.7972\n",
      "Epoch 3/10\n",
      "282/282 [==============================] - 14s 50ms/step - loss: 0.4149 - accuracy: 0.8227 - val_loss: 0.4981 - val_accuracy: 0.7912\n",
      "Epoch 4/10\n",
      "282/282 [==============================] - 14s 51ms/step - loss: 0.3963 - accuracy: 0.8309 - val_loss: 0.4754 - val_accuracy: 0.8012\n",
      "Epoch 5/10\n",
      "282/282 [==============================] - 14s 51ms/step - loss: 0.3791 - accuracy: 0.8493 - val_loss: 0.4912 - val_accuracy: 0.7879\n",
      "Epoch 6/10\n",
      "282/282 [==============================] - 14s 50ms/step - loss: 0.3644 - accuracy: 0.8522 - val_loss: 0.4880 - val_accuracy: 0.7945\n",
      "Epoch 7/10\n",
      "282/282 [==============================] - 14s 50ms/step - loss: 0.3386 - accuracy: 0.8672 - val_loss: 0.5726 - val_accuracy: 0.7719\n",
      "Epoch 7: early stopping\n",
      "94/94 [==============================] - 3s 14ms/step - loss: 0.5130 - accuracy: 0.7826\n",
      "basemodel__lr=0.0005__batch_size=16__optimizer=adamw__class_weights=none__dropout=0.1 completed.\n",
      "\n",
      "*********************    Session 6/24 ended    ***********************\n",
      "\n",
      "\n",
      "********************    Session 7/24 started    **********************\n",
      "\n",
      "Values are set: Optimizer: adamw | Learning Rate: 0.0005 | Batch Size: 16 | Class Weights: balanced | Dropout Rate: 0.4\n",
      "Model: \"sequential_125\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_125 (Embedding)   (None, None, 200)         3514600   \n",
      "                                                                 \n",
      " dropout_250 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " bidirectional_345 (Bidirect  (None, None, 256)        336896    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_346 (Bidirect  (None, None, 128)        164352    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_250 (Dense)           (None, None, 64)          8256      \n",
      "                                                                 \n",
      " global_average_pooling1d_12  (None, 64)               0         \n",
      " 5 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dropout_251 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_251 (Dense)           (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,024,234\n",
      "Trainable params: 509,634\n",
      "Non-trainable params: 3,514,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "basemodel__lr=0.0005__batch_size=16__optimizer=adamw__class_weights=balanced__dropout=0.4 starting...\n",
      "Epoch 1/10\n",
      "282/282 [==============================] - 22s 56ms/step - loss: 0.5785 - accuracy: 0.6953 - val_loss: 0.5359 - val_accuracy: 0.7653\n",
      "Epoch 2/10\n",
      "282/282 [==============================] - 13s 47ms/step - loss: 0.4661 - accuracy: 0.7977 - val_loss: 0.4809 - val_accuracy: 0.7793\n",
      "Epoch 3/10\n",
      "282/282 [==============================] - 13s 47ms/step - loss: 0.4483 - accuracy: 0.8074 - val_loss: 0.4617 - val_accuracy: 0.7926\n",
      "Epoch 4/10\n",
      "282/282 [==============================] - 13s 48ms/step - loss: 0.4271 - accuracy: 0.8098 - val_loss: 0.4784 - val_accuracy: 0.7952\n",
      "Epoch 5/10\n",
      "282/282 [==============================] - 14s 48ms/step - loss: 0.4182 - accuracy: 0.8203 - val_loss: 0.4660 - val_accuracy: 0.7979\n",
      "Epoch 6/10\n",
      "282/282 [==============================] - 14s 48ms/step - loss: 0.4066 - accuracy: 0.8254 - val_loss: 0.4720 - val_accuracy: 0.8052\n",
      "Epoch 7/10\n",
      "282/282 [==============================] - 14s 49ms/step - loss: 0.3978 - accuracy: 0.8320 - val_loss: 0.4875 - val_accuracy: 0.7965\n",
      "Epoch 8/10\n",
      "282/282 [==============================] - 14s 49ms/step - loss: 0.3804 - accuracy: 0.8409 - val_loss: 0.4662 - val_accuracy: 0.8039\n",
      "Epoch 8: early stopping\n",
      "94/94 [==============================] - 3s 13ms/step - loss: 0.4325 - accuracy: 0.8191\n",
      "basemodel__lr=0.0005__batch_size=16__optimizer=adamw__class_weights=balanced__dropout=0.4 completed.\n",
      "\n",
      "*********************    Session 7/24 ended    ***********************\n",
      "\n",
      "\n",
      "********************    Session 8/24 started    **********************\n",
      "\n",
      "Values are set: Optimizer: adamw | Learning Rate: 0.0005 | Batch Size: 16 | Class Weights: none | Dropout Rate: 0.4\n",
      "Model: \"sequential_126\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_126 (Embedding)   (None, None, 200)         3514600   \n",
      "                                                                 \n",
      " dropout_252 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " bidirectional_347 (Bidirect  (None, None, 256)        336896    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_348 (Bidirect  (None, None, 128)        164352    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_252 (Dense)           (None, None, 64)          8256      \n",
      "                                                                 \n",
      " global_average_pooling1d_12  (None, 64)               0         \n",
      " 6 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dropout_253 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_253 (Dense)           (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,024,234\n",
      "Trainable params: 509,634\n",
      "Non-trainable params: 3,514,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "basemodel__lr=0.0005__batch_size=16__optimizer=adamw__class_weights=none__dropout=0.4 starting...\n",
      "Epoch 1/10\n",
      "282/282 [==============================] - 22s 55ms/step - loss: 0.5763 - accuracy: 0.7008 - val_loss: 0.6724 - val_accuracy: 0.6822\n",
      "Epoch 2/10\n",
      "282/282 [==============================] - 13s 47ms/step - loss: 0.4734 - accuracy: 0.7917 - val_loss: 0.4715 - val_accuracy: 0.7872\n",
      "Epoch 3/10\n",
      "282/282 [==============================] - 13s 47ms/step - loss: 0.4467 - accuracy: 0.8067 - val_loss: 0.4881 - val_accuracy: 0.7852\n",
      "Epoch 4/10\n",
      "282/282 [==============================] - 14s 48ms/step - loss: 0.4295 - accuracy: 0.8223 - val_loss: 0.4840 - val_accuracy: 0.7939\n",
      "Epoch 5/10\n",
      "282/282 [==============================] - 14s 49ms/step - loss: 0.4219 - accuracy: 0.8218 - val_loss: 0.4834 - val_accuracy: 0.7952\n",
      "Epoch 6/10\n",
      "282/282 [==============================] - 14s 48ms/step - loss: 0.4099 - accuracy: 0.8258 - val_loss: 0.4705 - val_accuracy: 0.7926\n",
      "Epoch 7/10\n",
      "282/282 [==============================] - 14s 49ms/step - loss: 0.3980 - accuracy: 0.8358 - val_loss: 0.4757 - val_accuracy: 0.7992\n",
      "Epoch 8/10\n",
      "282/282 [==============================] - 14s 49ms/step - loss: 0.3955 - accuracy: 0.8340 - val_loss: 0.4401 - val_accuracy: 0.8052\n",
      "Epoch 9/10\n",
      "282/282 [==============================] - 14s 49ms/step - loss: 0.3784 - accuracy: 0.8380 - val_loss: 0.4509 - val_accuracy: 0.8032\n",
      "Epoch 10/10\n",
      "282/282 [==============================] - 14s 49ms/step - loss: 0.3757 - accuracy: 0.8453 - val_loss: 0.4471 - val_accuracy: 0.8032\n",
      "94/94 [==============================] - 3s 13ms/step - loss: 0.4179 - accuracy: 0.8211\n",
      "basemodel__lr=0.0005__batch_size=16__optimizer=adamw__class_weights=none__dropout=0.4 completed.\n",
      "\n",
      "*********************    Session 8/24 ended    ***********************\n",
      "\n",
      "\n",
      "********************    Session 9/24 started    **********************\n",
      "\n",
      "Values are set: Optimizer: adamw | Learning Rate: 0.0001 | Batch Size: 32 | Class Weights: balanced | Dropout Rate: 0.1\n",
      "Model: \"sequential_127\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_127 (Embedding)   (None, None, 200)         3514600   \n",
      "                                                                 \n",
      " dropout_254 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " bidirectional_349 (Bidirect  (None, None, 256)        336896    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_350 (Bidirect  (None, None, 128)        164352    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_254 (Dense)           (None, None, 64)          8256      \n",
      "                                                                 \n",
      " global_average_pooling1d_12  (None, 64)               0         \n",
      " 7 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dropout_255 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_255 (Dense)           (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,024,234\n",
      "Trainable params: 509,634\n",
      "Non-trainable params: 3,514,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "basemodel__lr=0.0001__batch_size=32__optimizer=adamw__class_weights=balanced__dropout=0.1 starting...\n",
      "Epoch 1/10\n",
      "141/141 [==============================] - 19s 86ms/step - loss: 0.6751 - accuracy: 0.5838 - val_loss: 0.5743 - val_accuracy: 0.7560\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - 10s 68ms/step - loss: 0.4934 - accuracy: 0.7799 - val_loss: 0.4588 - val_accuracy: 0.7819\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - 10s 69ms/step - loss: 0.4439 - accuracy: 0.8012 - val_loss: 0.4493 - val_accuracy: 0.7892\n",
      "Epoch 4/10\n",
      "141/141 [==============================] - 10s 71ms/step - loss: 0.4275 - accuracy: 0.8109 - val_loss: 0.4463 - val_accuracy: 0.7939\n",
      "Epoch 5/10\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 0.4202 - accuracy: 0.8141 - val_loss: 0.4467 - val_accuracy: 0.7959\n",
      "Epoch 6/10\n",
      "141/141 [==============================] - 10s 73ms/step - loss: 0.4159 - accuracy: 0.8129 - val_loss: 0.4467 - val_accuracy: 0.7999\n",
      "Epoch 7/10\n",
      "141/141 [==============================] - 10s 73ms/step - loss: 0.4090 - accuracy: 0.8220 - val_loss: 0.4496 - val_accuracy: 0.7985\n",
      "Epoch 8/10\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 0.4019 - accuracy: 0.8223 - val_loss: 0.4495 - val_accuracy: 0.8012\n",
      "Epoch 9/10\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 0.3968 - accuracy: 0.8265 - val_loss: 0.4446 - val_accuracy: 0.7992\n",
      "Epoch 10/10\n",
      "141/141 [==============================] - 11s 75ms/step - loss: 0.3927 - accuracy: 0.8298 - val_loss: 0.4445 - val_accuracy: 0.7979\n",
      "47/47 [==============================] - 2s 23ms/step - loss: 0.4302 - accuracy: 0.8092\n",
      "basemodel__lr=0.0001__batch_size=32__optimizer=adamw__class_weights=balanced__dropout=0.1 completed.\n",
      "\n",
      "*********************    Session 9/24 ended    ***********************\n",
      "\n",
      "\n",
      "********************    Session 10/24 started    **********************\n",
      "\n",
      "Values are set: Optimizer: adamw | Learning Rate: 0.0001 | Batch Size: 32 | Class Weights: none | Dropout Rate: 0.1\n",
      "Model: \"sequential_128\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_128 (Embedding)   (None, None, 200)         3514600   \n",
      "                                                                 \n",
      " dropout_256 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " bidirectional_351 (Bidirect  (None, None, 256)        336896    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_352 (Bidirect  (None, None, 128)        164352    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_256 (Dense)           (None, None, 64)          8256      \n",
      "                                                                 \n",
      " global_average_pooling1d_12  (None, 64)               0         \n",
      " 8 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dropout_257 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_257 (Dense)           (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,024,234\n",
      "Trainable params: 509,634\n",
      "Non-trainable params: 3,514,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "basemodel__lr=0.0001__batch_size=32__optimizer=adamw__class_weights=none__dropout=0.1 starting...\n",
      "Epoch 1/10\n",
      "141/141 [==============================] - 21s 96ms/step - loss: 0.6586 - accuracy: 0.6201 - val_loss: 0.5580 - val_accuracy: 0.7420\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 0.4963 - accuracy: 0.7753 - val_loss: 0.4672 - val_accuracy: 0.7779\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - 11s 76ms/step - loss: 0.4439 - accuracy: 0.8050 - val_loss: 0.4501 - val_accuracy: 0.7839\n",
      "Epoch 4/10\n",
      "141/141 [==============================] - 10s 71ms/step - loss: 0.4296 - accuracy: 0.8125 - val_loss: 0.4417 - val_accuracy: 0.7906\n",
      "Epoch 5/10\n",
      "141/141 [==============================] - 10s 71ms/step - loss: 0.4207 - accuracy: 0.8169 - val_loss: 0.4393 - val_accuracy: 0.7965\n",
      "Epoch 6/10\n",
      "141/141 [==============================] - 10s 71ms/step - loss: 0.4145 - accuracy: 0.8234 - val_loss: 0.4390 - val_accuracy: 0.8012\n",
      "Epoch 7/10\n",
      "141/141 [==============================] - 10s 71ms/step - loss: 0.4101 - accuracy: 0.8211 - val_loss: 0.4383 - val_accuracy: 0.8032\n",
      "Epoch 8/10\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 0.4085 - accuracy: 0.8209 - val_loss: 0.4388 - val_accuracy: 0.8025\n",
      "Epoch 9/10\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 0.4023 - accuracy: 0.8218 - val_loss: 0.4375 - val_accuracy: 0.8019\n",
      "Epoch 10/10\n",
      "141/141 [==============================] - 10s 71ms/step - loss: 0.4002 - accuracy: 0.8251 - val_loss: 0.4382 - val_accuracy: 0.8005\n",
      "47/47 [==============================] - 2s 22ms/step - loss: 0.4210 - accuracy: 0.8105\n",
      "basemodel__lr=0.0001__batch_size=32__optimizer=adamw__class_weights=none__dropout=0.1 completed.\n",
      "\n",
      "*********************    Session 10/24 ended    ***********************\n",
      "\n",
      "\n",
      "********************    Session 11/24 started    **********************\n",
      "\n",
      "Values are set: Optimizer: adamw | Learning Rate: 0.0001 | Batch Size: 32 | Class Weights: balanced | Dropout Rate: 0.4\n",
      "Model: \"sequential_129\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_129 (Embedding)   (None, None, 200)         3514600   \n",
      "                                                                 \n",
      " dropout_258 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " bidirectional_353 (Bidirect  (None, None, 256)        336896    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_354 (Bidirect  (None, None, 128)        164352    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_258 (Dense)           (None, None, 64)          8256      \n",
      "                                                                 \n",
      " global_average_pooling1d_12  (None, 64)               0         \n",
      " 9 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dropout_259 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_259 (Dense)           (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,024,234\n",
      "Trainable params: 509,634\n",
      "Non-trainable params: 3,514,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "basemodel__lr=0.0001__batch_size=32__optimizer=adamw__class_weights=balanced__dropout=0.4 starting...\n",
      "Epoch 1/10\n",
      "141/141 [==============================] - 19s 86ms/step - loss: 0.6853 - accuracy: 0.5652 - val_loss: 0.6198 - val_accuracy: 0.6602\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - 10s 69ms/step - loss: 0.5449 - accuracy: 0.7440 - val_loss: 0.4716 - val_accuracy: 0.7739\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - 10s 69ms/step - loss: 0.4792 - accuracy: 0.7895 - val_loss: 0.4623 - val_accuracy: 0.7899\n",
      "Epoch 4/10\n",
      "141/141 [==============================] - 10s 70ms/step - loss: 0.4617 - accuracy: 0.7954 - val_loss: 0.4483 - val_accuracy: 0.7959\n",
      "Epoch 5/10\n",
      "141/141 [==============================] - 11s 75ms/step - loss: 0.4582 - accuracy: 0.7979 - val_loss: 0.4509 - val_accuracy: 0.7992\n",
      "Epoch 6/10\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 0.4406 - accuracy: 0.8010 - val_loss: 0.4506 - val_accuracy: 0.8025\n",
      "Epoch 7/10\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 0.4385 - accuracy: 0.8090 - val_loss: 0.4463 - val_accuracy: 0.7992\n",
      "Epoch 8/10\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 0.4406 - accuracy: 0.8078 - val_loss: 0.4414 - val_accuracy: 0.7999\n",
      "Epoch 9/10\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 0.4417 - accuracy: 0.8032 - val_loss: 0.4415 - val_accuracy: 0.8012\n",
      "Epoch 10/10\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 0.4328 - accuracy: 0.8116 - val_loss: 0.4404 - val_accuracy: 0.8025\n",
      "47/47 [==============================] - 2s 22ms/step - loss: 0.4237 - accuracy: 0.8178\n",
      "basemodel__lr=0.0001__batch_size=32__optimizer=adamw__class_weights=balanced__dropout=0.4 completed.\n",
      "\n",
      "*********************    Session 11/24 ended    ***********************\n",
      "\n",
      "\n",
      "********************    Session 12/24 started    **********************\n",
      "\n",
      "Values are set: Optimizer: adamw | Learning Rate: 0.0001 | Batch Size: 32 | Class Weights: none | Dropout Rate: 0.4\n",
      "Model: \"sequential_130\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_130 (Embedding)   (None, None, 200)         3514600   \n",
      "                                                                 \n",
      " dropout_260 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " bidirectional_355 (Bidirect  (None, None, 256)        336896    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_356 (Bidirect  (None, None, 128)        164352    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_260 (Dense)           (None, None, 64)          8256      \n",
      "                                                                 \n",
      " global_average_pooling1d_13  (None, 64)               0         \n",
      " 0 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dropout_261 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_261 (Dense)           (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,024,234\n",
      "Trainable params: 509,634\n",
      "Non-trainable params: 3,514,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "basemodel__lr=0.0001__batch_size=32__optimizer=adamw__class_weights=none__dropout=0.4 starting...\n",
      "Epoch 1/10\n",
      "141/141 [==============================] - 20s 90ms/step - loss: 0.6704 - accuracy: 0.6219 - val_loss: 0.5846 - val_accuracy: 0.7460\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 0.5274 - accuracy: 0.7666 - val_loss: 0.4846 - val_accuracy: 0.7693\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - 10s 74ms/step - loss: 0.4736 - accuracy: 0.7914 - val_loss: 0.4605 - val_accuracy: 0.7832\n",
      "Epoch 4/10\n",
      "141/141 [==============================] - 11s 75ms/step - loss: 0.4537 - accuracy: 0.8039 - val_loss: 0.4493 - val_accuracy: 0.7859\n",
      "Epoch 5/10\n",
      "141/141 [==============================] - 10s 73ms/step - loss: 0.4535 - accuracy: 0.8050 - val_loss: 0.4449 - val_accuracy: 0.7926\n",
      "Epoch 6/10\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 0.4459 - accuracy: 0.8085 - val_loss: 0.4395 - val_accuracy: 0.7912\n",
      "Epoch 7/10\n",
      "141/141 [==============================] - 10s 71ms/step - loss: 0.4412 - accuracy: 0.8061 - val_loss: 0.4411 - val_accuracy: 0.7906\n",
      "Epoch 8/10\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 0.4387 - accuracy: 0.8096 - val_loss: 0.4389 - val_accuracy: 0.7906\n",
      "Epoch 9/10\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 0.4372 - accuracy: 0.8090 - val_loss: 0.4380 - val_accuracy: 0.7939\n",
      "Epoch 10/10\n",
      "141/141 [==============================] - 10s 71ms/step - loss: 0.4318 - accuracy: 0.8145 - val_loss: 0.4385 - val_accuracy: 0.7939\n",
      "47/47 [==============================] - 2s 21ms/step - loss: 0.4154 - accuracy: 0.8172\n",
      "basemodel__lr=0.0001__batch_size=32__optimizer=adamw__class_weights=none__dropout=0.4 completed.\n",
      "\n",
      "*********************    Session 12/24 ended    ***********************\n",
      "\n",
      "\n",
      "********************    Session 13/24 started    **********************\n",
      "\n",
      "Values are set: Optimizer: adamw | Learning Rate: 0.0005 | Batch Size: 32 | Class Weights: balanced | Dropout Rate: 0.1\n",
      "Model: \"sequential_131\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_131 (Embedding)   (None, None, 200)         3514600   \n",
      "                                                                 \n",
      " dropout_262 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " bidirectional_357 (Bidirect  (None, None, 256)        336896    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_358 (Bidirect  (None, None, 128)        164352    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_262 (Dense)           (None, None, 64)          8256      \n",
      "                                                                 \n",
      " global_average_pooling1d_13  (None, 64)               0         \n",
      " 1 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dropout_263 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_263 (Dense)           (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,024,234\n",
      "Trainable params: 509,634\n",
      "Non-trainable params: 3,514,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "basemodel__lr=0.0005__batch_size=32__optimizer=adamw__class_weights=balanced__dropout=0.1 starting...\n",
      "Epoch 1/10\n",
      "141/141 [==============================] - 20s 90ms/step - loss: 0.5714 - accuracy: 0.7028 - val_loss: 0.4850 - val_accuracy: 0.7699\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 0.4488 - accuracy: 0.8010 - val_loss: 0.4906 - val_accuracy: 0.7766\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - 10s 74ms/step - loss: 0.4166 - accuracy: 0.8145 - val_loss: 0.5148 - val_accuracy: 0.7706\n",
      "Epoch 4/10\n",
      "141/141 [==============================] - 11s 75ms/step - loss: 0.3998 - accuracy: 0.8260 - val_loss: 0.4826 - val_accuracy: 0.7859\n",
      "Epoch 5/10\n",
      "141/141 [==============================] - 10s 73ms/step - loss: 0.3808 - accuracy: 0.8340 - val_loss: 0.4848 - val_accuracy: 0.7886\n",
      "Epoch 6/10\n",
      "141/141 [==============================] - 10s 73ms/step - loss: 0.3704 - accuracy: 0.8400 - val_loss: 0.4969 - val_accuracy: 0.7959\n",
      "Epoch 7/10\n",
      "141/141 [==============================] - 10s 73ms/step - loss: 0.3539 - accuracy: 0.8504 - val_loss: 0.4957 - val_accuracy: 0.7985\n",
      "Epoch 8/10\n",
      "141/141 [==============================] - 10s 74ms/step - loss: 0.3443 - accuracy: 0.8548 - val_loss: 0.4914 - val_accuracy: 0.7992\n",
      "Epoch 9/10\n",
      "141/141 [==============================] - 10s 73ms/step - loss: 0.3280 - accuracy: 0.8593 - val_loss: 0.5046 - val_accuracy: 0.8065\n",
      "Epoch 9: early stopping\n",
      "47/47 [==============================] - 2s 22ms/step - loss: 0.4909 - accuracy: 0.8039\n",
      "basemodel__lr=0.0005__batch_size=32__optimizer=adamw__class_weights=balanced__dropout=0.1 completed.\n",
      "\n",
      "*********************    Session 13/24 ended    ***********************\n",
      "\n",
      "\n",
      "********************    Session 14/24 started    **********************\n",
      "\n",
      "Values are set: Optimizer: adamw | Learning Rate: 0.0005 | Batch Size: 32 | Class Weights: none | Dropout Rate: 0.1\n",
      "Model: \"sequential_132\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_132 (Embedding)   (None, None, 200)         3514600   \n",
      "                                                                 \n",
      " dropout_264 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " bidirectional_359 (Bidirect  (None, None, 256)        336896    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_360 (Bidirect  (None, None, 128)        164352    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_264 (Dense)           (None, None, 64)          8256      \n",
      "                                                                 \n",
      " global_average_pooling1d_13  (None, 64)               0         \n",
      " 2 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dropout_265 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_265 (Dense)           (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,024,234\n",
      "Trainable params: 509,634\n",
      "Non-trainable params: 3,514,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "basemodel__lr=0.0005__batch_size=32__optimizer=adamw__class_weights=none__dropout=0.1 starting...\n",
      "Epoch 1/10\n",
      "141/141 [==============================] - 19s 87ms/step - loss: 0.5649 - accuracy: 0.7128 - val_loss: 0.6131 - val_accuracy: 0.7041\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - 10s 68ms/step - loss: 0.4518 - accuracy: 0.7983 - val_loss: 0.5076 - val_accuracy: 0.7739\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - 10s 70ms/step - loss: 0.4191 - accuracy: 0.8220 - val_loss: 0.5001 - val_accuracy: 0.7746\n",
      "Epoch 4/10\n",
      "141/141 [==============================] - 10s 70ms/step - loss: 0.4006 - accuracy: 0.8249 - val_loss: 0.5062 - val_accuracy: 0.7766\n",
      "Epoch 5/10\n",
      "141/141 [==============================] - 10s 71ms/step - loss: 0.3816 - accuracy: 0.8389 - val_loss: 0.5086 - val_accuracy: 0.7926\n",
      "Epoch 6/10\n",
      "141/141 [==============================] - 10s 71ms/step - loss: 0.3656 - accuracy: 0.8411 - val_loss: 0.4895 - val_accuracy: 0.8065\n",
      "Epoch 7/10\n",
      "141/141 [==============================] - 10s 73ms/step - loss: 0.3547 - accuracy: 0.8524 - val_loss: 0.4827 - val_accuracy: 0.8092\n",
      "Epoch 8/10\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 0.3435 - accuracy: 0.8575 - val_loss: 0.4693 - val_accuracy: 0.8052\n",
      "Epoch 9/10\n",
      "141/141 [==============================] - 10s 71ms/step - loss: 0.3317 - accuracy: 0.8624 - val_loss: 0.4809 - val_accuracy: 0.8019\n",
      "Epoch 10/10\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 0.3176 - accuracy: 0.8717 - val_loss: 0.4902 - val_accuracy: 0.7999\n",
      "47/47 [==============================] - 2s 22ms/step - loss: 0.4751 - accuracy: 0.8052\n",
      "basemodel__lr=0.0005__batch_size=32__optimizer=adamw__class_weights=none__dropout=0.1 completed.\n",
      "\n",
      "*********************    Session 14/24 ended    ***********************\n",
      "\n",
      "\n",
      "********************    Session 15/24 started    **********************\n",
      "\n",
      "Values are set: Optimizer: adamw | Learning Rate: 0.0005 | Batch Size: 32 | Class Weights: balanced | Dropout Rate: 0.4\n",
      "Model: \"sequential_133\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_133 (Embedding)   (None, None, 200)         3514600   \n",
      "                                                                 \n",
      " dropout_266 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " bidirectional_361 (Bidirect  (None, None, 256)        336896    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_362 (Bidirect  (None, None, 128)        164352    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_266 (Dense)           (None, None, 64)          8256      \n",
      "                                                                 \n",
      " global_average_pooling1d_13  (None, 64)               0         \n",
      " 3 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dropout_267 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_267 (Dense)           (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,024,234\n",
      "Trainable params: 509,634\n",
      "Non-trainable params: 3,514,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "basemodel__lr=0.0005__batch_size=32__optimizer=adamw__class_weights=balanced__dropout=0.4 starting...\n",
      "Epoch 1/10\n",
      "141/141 [==============================] - 19s 86ms/step - loss: 0.6008 - accuracy: 0.6715 - val_loss: 0.4977 - val_accuracy: 0.7739\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - 10s 68ms/step - loss: 0.4712 - accuracy: 0.7881 - val_loss: 0.4843 - val_accuracy: 0.7859\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - 10s 70ms/step - loss: 0.4477 - accuracy: 0.8005 - val_loss: 0.4770 - val_accuracy: 0.7799\n",
      "Epoch 4/10\n",
      "141/141 [==============================] - 10s 70ms/step - loss: 0.4360 - accuracy: 0.8085 - val_loss: 0.4473 - val_accuracy: 0.7939\n",
      "Epoch 5/10\n",
      "141/141 [==============================] - 10s 71ms/step - loss: 0.4187 - accuracy: 0.8167 - val_loss: 0.4505 - val_accuracy: 0.7979\n",
      "Epoch 6/10\n",
      "141/141 [==============================] - 10s 70ms/step - loss: 0.4134 - accuracy: 0.8229 - val_loss: 0.4384 - val_accuracy: 0.8012\n",
      "Epoch 7/10\n",
      "141/141 [==============================] - 10s 69ms/step - loss: 0.3944 - accuracy: 0.8280 - val_loss: 0.4428 - val_accuracy: 0.7992\n",
      "Epoch 8/10\n",
      "141/141 [==============================] - 10s 69ms/step - loss: 0.3997 - accuracy: 0.8287 - val_loss: 0.4380 - val_accuracy: 0.8078\n",
      "Epoch 9/10\n",
      "141/141 [==============================] - 10s 70ms/step - loss: 0.3849 - accuracy: 0.8329 - val_loss: 0.4453 - val_accuracy: 0.8039\n",
      "Epoch 10/10\n",
      "141/141 [==============================] - 10s 70ms/step - loss: 0.3813 - accuracy: 0.8360 - val_loss: 0.4399 - val_accuracy: 0.8045\n",
      "47/47 [==============================] - 2s 22ms/step - loss: 0.4286 - accuracy: 0.8191\n",
      "basemodel__lr=0.0005__batch_size=32__optimizer=adamw__class_weights=balanced__dropout=0.4 completed.\n",
      "\n",
      "*********************    Session 15/24 ended    ***********************\n",
      "\n",
      "\n",
      "********************    Session 16/24 started    **********************\n",
      "\n",
      "Values are set: Optimizer: adamw | Learning Rate: 0.0005 | Batch Size: 32 | Class Weights: none | Dropout Rate: 0.4\n",
      "Model: \"sequential_134\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_134 (Embedding)   (None, None, 200)         3514600   \n",
      "                                                                 \n",
      " dropout_268 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " bidirectional_363 (Bidirect  (None, None, 256)        336896    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_364 (Bidirect  (None, None, 128)        164352    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_268 (Dense)           (None, None, 64)          8256      \n",
      "                                                                 \n",
      " global_average_pooling1d_13  (None, 64)               0         \n",
      " 4 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dropout_269 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_269 (Dense)           (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,024,234\n",
      "Trainable params: 509,634\n",
      "Non-trainable params: 3,514,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "basemodel__lr=0.0005__batch_size=32__optimizer=adamw__class_weights=none__dropout=0.4 starting...\n",
      "Epoch 1/10\n",
      "141/141 [==============================] - 19s 86ms/step - loss: 0.5956 - accuracy: 0.6806 - val_loss: 0.5343 - val_accuracy: 0.7527\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - 10s 68ms/step - loss: 0.4744 - accuracy: 0.7877 - val_loss: 0.5402 - val_accuracy: 0.7759\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - 10s 70ms/step - loss: 0.4610 - accuracy: 0.8021 - val_loss: 0.4598 - val_accuracy: 0.7912\n",
      "Epoch 4/10\n",
      "141/141 [==============================] - 10s 70ms/step - loss: 0.4322 - accuracy: 0.8030 - val_loss: 0.4520 - val_accuracy: 0.7999\n",
      "Epoch 5/10\n",
      "141/141 [==============================] - 10s 70ms/step - loss: 0.4302 - accuracy: 0.8118 - val_loss: 0.4432 - val_accuracy: 0.8012\n",
      "Epoch 6/10\n",
      "141/141 [==============================] - 10s 69ms/step - loss: 0.4087 - accuracy: 0.8242 - val_loss: 0.4299 - val_accuracy: 0.8045\n",
      "Epoch 7/10\n",
      "141/141 [==============================] - 10s 73ms/step - loss: 0.4089 - accuracy: 0.8229 - val_loss: 0.4338 - val_accuracy: 0.8039\n",
      "Epoch 8/10\n",
      "141/141 [==============================] - 10s 70ms/step - loss: 0.3952 - accuracy: 0.8276 - val_loss: 0.4314 - val_accuracy: 0.8045\n",
      "Epoch 9/10\n",
      "141/141 [==============================] - 10s 72ms/step - loss: 0.3887 - accuracy: 0.8347 - val_loss: 0.4432 - val_accuracy: 0.8078\n",
      "Epoch 10/10\n",
      "141/141 [==============================] - 10s 71ms/step - loss: 0.3893 - accuracy: 0.8305 - val_loss: 0.4351 - val_accuracy: 0.8052\n",
      "47/47 [==============================] - 2s 21ms/step - loss: 0.4204 - accuracy: 0.8191\n",
      "basemodel__lr=0.0005__batch_size=32__optimizer=adamw__class_weights=none__dropout=0.4 completed.\n",
      "\n",
      "*********************    Session 16/24 ended    ***********************\n",
      "\n",
      "\n",
      "********************    Session 17/24 started    **********************\n",
      "\n",
      "Values are set: Optimizer: adamw | Learning Rate: 0.0001 | Batch Size: 64 | Class Weights: balanced | Dropout Rate: 0.1\n",
      "Model: \"sequential_135\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_135 (Embedding)   (None, None, 200)         3514600   \n",
      "                                                                 \n",
      " dropout_270 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " bidirectional_365 (Bidirect  (None, None, 256)        336896    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_366 (Bidirect  (None, None, 128)        164352    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_270 (Dense)           (None, None, 64)          8256      \n",
      "                                                                 \n",
      " global_average_pooling1d_13  (None, 64)               0         \n",
      " 5 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dropout_271 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_271 (Dense)           (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,024,234\n",
      "Trainable params: 509,634\n",
      "Non-trainable params: 3,514,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "basemodel__lr=0.0001__batch_size=64__optimizer=adamw__class_weights=balanced__dropout=0.1 starting...\n",
      "Epoch 1/10\n",
      "70/70 [==============================] - 16s 140ms/step - loss: 0.7037 - accuracy: 0.5205 - val_loss: 0.6584 - val_accuracy: 0.5795\n",
      "Epoch 2/10\n",
      "70/70 [==============================] - 7s 101ms/step - loss: 0.5837 - accuracy: 0.7074 - val_loss: 0.5032 - val_accuracy: 0.7704\n",
      "Epoch 3/10\n",
      "70/70 [==============================] - 7s 104ms/step - loss: 0.4804 - accuracy: 0.7897 - val_loss: 0.4684 - val_accuracy: 0.7874\n",
      "Epoch 4/10\n",
      "70/70 [==============================] - 7s 102ms/step - loss: 0.4541 - accuracy: 0.7973 - val_loss: 0.4542 - val_accuracy: 0.7935\n",
      "Epoch 5/10\n",
      "70/70 [==============================] - 7s 102ms/step - loss: 0.4369 - accuracy: 0.8089 - val_loss: 0.4502 - val_accuracy: 0.7921\n",
      "Epoch 6/10\n",
      "70/70 [==============================] - 7s 103ms/step - loss: 0.4261 - accuracy: 0.8152 - val_loss: 0.4475 - val_accuracy: 0.7914\n",
      "Epoch 7/10\n",
      "70/70 [==============================] - 7s 104ms/step - loss: 0.4220 - accuracy: 0.8170 - val_loss: 0.4444 - val_accuracy: 0.7901\n",
      "Epoch 8/10\n",
      "70/70 [==============================] - 7s 103ms/step - loss: 0.4200 - accuracy: 0.8183 - val_loss: 0.4425 - val_accuracy: 0.7914\n",
      "Epoch 9/10\n",
      "70/70 [==============================] - 7s 105ms/step - loss: 0.4135 - accuracy: 0.8228 - val_loss: 0.4432 - val_accuracy: 0.7942\n",
      "Epoch 10/10\n",
      "70/70 [==============================] - 7s 104ms/step - loss: 0.4114 - accuracy: 0.8228 - val_loss: 0.4420 - val_accuracy: 0.7901\n",
      "23/23 [==============================] - 2s 33ms/step - loss: 0.4357 - accuracy: 0.8098\n",
      "basemodel__lr=0.0001__batch_size=64__optimizer=adamw__class_weights=balanced__dropout=0.1 completed.\n",
      "\n",
      "*********************    Session 17/24 ended    ***********************\n",
      "\n",
      "\n",
      "********************    Session 18/24 started    **********************\n",
      "\n",
      "Values are set: Optimizer: adamw | Learning Rate: 0.0001 | Batch Size: 64 | Class Weights: none | Dropout Rate: 0.1\n",
      "Model: \"sequential_136\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_136 (Embedding)   (None, None, 200)         3514600   \n",
      "                                                                 \n",
      " dropout_272 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " bidirectional_367 (Bidirect  (None, None, 256)        336896    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_368 (Bidirect  (None, None, 128)        164352    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_272 (Dense)           (None, None, 64)          8256      \n",
      "                                                                 \n",
      " global_average_pooling1d_13  (None, 64)               0         \n",
      " 6 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dropout_273 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_273 (Dense)           (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,024,234\n",
      "Trainable params: 509,634\n",
      "Non-trainable params: 3,514,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "basemodel__lr=0.0001__batch_size=64__optimizer=adamw__class_weights=none__dropout=0.1 starting...\n",
      "Epoch 1/10\n",
      "70/70 [==============================] - 16s 132ms/step - loss: 0.6855 - accuracy: 0.5391 - val_loss: 0.6487 - val_accuracy: 0.6821\n",
      "Epoch 2/10\n",
      "70/70 [==============================] - 7s 95ms/step - loss: 0.5730 - accuracy: 0.7551 - val_loss: 0.4880 - val_accuracy: 0.7683\n",
      "Epoch 3/10\n",
      "70/70 [==============================] - 7s 96ms/step - loss: 0.4780 - accuracy: 0.7862 - val_loss: 0.4529 - val_accuracy: 0.7887\n",
      "Epoch 4/10\n",
      "70/70 [==============================] - 7s 97ms/step - loss: 0.4508 - accuracy: 0.7993 - val_loss: 0.4386 - val_accuracy: 0.7962\n",
      "Epoch 5/10\n",
      "70/70 [==============================] - 7s 98ms/step - loss: 0.4367 - accuracy: 0.8074 - val_loss: 0.4354 - val_accuracy: 0.7976\n",
      "Epoch 6/10\n",
      "70/70 [==============================] - 7s 98ms/step - loss: 0.4275 - accuracy: 0.8145 - val_loss: 0.4320 - val_accuracy: 0.8010\n",
      "Epoch 7/10\n",
      "70/70 [==============================] - 7s 98ms/step - loss: 0.4194 - accuracy: 0.8167 - val_loss: 0.4299 - val_accuracy: 0.8016\n",
      "Epoch 8/10\n",
      "70/70 [==============================] - 7s 97ms/step - loss: 0.4136 - accuracy: 0.8232 - val_loss: 0.4298 - val_accuracy: 0.7996\n",
      "Epoch 9/10\n",
      "70/70 [==============================] - 7s 97ms/step - loss: 0.4132 - accuracy: 0.8266 - val_loss: 0.4293 - val_accuracy: 0.8003\n",
      "Epoch 10/10\n",
      "70/70 [==============================] - 7s 98ms/step - loss: 0.4119 - accuracy: 0.8199 - val_loss: 0.4300 - val_accuracy: 0.8023\n",
      "23/23 [==============================] - 2s 31ms/step - loss: 0.4189 - accuracy: 0.8207\n",
      "basemodel__lr=0.0001__batch_size=64__optimizer=adamw__class_weights=none__dropout=0.1 completed.\n",
      "\n",
      "*********************    Session 18/24 ended    ***********************\n",
      "\n",
      "\n",
      "********************    Session 19/24 started    **********************\n",
      "\n",
      "Values are set: Optimizer: adamw | Learning Rate: 0.0001 | Batch Size: 64 | Class Weights: balanced | Dropout Rate: 0.4\n",
      "Model: \"sequential_137\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_137 (Embedding)   (None, None, 200)         3514600   \n",
      "                                                                 \n",
      " dropout_274 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " bidirectional_369 (Bidirect  (None, None, 256)        336896    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_370 (Bidirect  (None, None, 128)        164352    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_274 (Dense)           (None, None, 64)          8256      \n",
      "                                                                 \n",
      " global_average_pooling1d_13  (None, 64)               0         \n",
      " 7 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dropout_275 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_275 (Dense)           (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,024,234\n",
      "Trainable params: 509,634\n",
      "Non-trainable params: 3,514,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "basemodel__lr=0.0001__batch_size=64__optimizer=adamw__class_weights=balanced__dropout=0.4 starting...\n",
      "Epoch 1/10\n",
      "70/70 [==============================] - 15s 128ms/step - loss: 0.7090 - accuracy: 0.5324 - val_loss: 0.6614 - val_accuracy: 0.5822\n",
      "Epoch 2/10\n",
      "70/70 [==============================] - 6s 93ms/step - loss: 0.6146 - accuracy: 0.6842 - val_loss: 0.5249 - val_accuracy: 0.7575\n",
      "Epoch 3/10\n",
      "70/70 [==============================] - 7s 94ms/step - loss: 0.5161 - accuracy: 0.7728 - val_loss: 0.4761 - val_accuracy: 0.7765\n",
      "Epoch 4/10\n",
      "70/70 [==============================] - 7s 95ms/step - loss: 0.4853 - accuracy: 0.7815 - val_loss: 0.4620 - val_accuracy: 0.7853\n",
      "Epoch 5/10\n",
      "70/70 [==============================] - 7s 95ms/step - loss: 0.4696 - accuracy: 0.7915 - val_loss: 0.4541 - val_accuracy: 0.7860\n",
      "Epoch 6/10\n",
      "70/70 [==============================] - 7s 97ms/step - loss: 0.4606 - accuracy: 0.7980 - val_loss: 0.4498 - val_accuracy: 0.7921\n",
      "Epoch 7/10\n",
      "70/70 [==============================] - 7s 96ms/step - loss: 0.4534 - accuracy: 0.8016 - val_loss: 0.4468 - val_accuracy: 0.7935\n",
      "Epoch 8/10\n",
      "70/70 [==============================] - 7s 96ms/step - loss: 0.4537 - accuracy: 0.8000 - val_loss: 0.4430 - val_accuracy: 0.7914\n",
      "Epoch 9/10\n",
      "70/70 [==============================] - 7s 96ms/step - loss: 0.4473 - accuracy: 0.8047 - val_loss: 0.4431 - val_accuracy: 0.7928\n",
      "Epoch 10/10\n",
      "70/70 [==============================] - 7s 95ms/step - loss: 0.4440 - accuracy: 0.8096 - val_loss: 0.4433 - val_accuracy: 0.7880\n",
      "23/23 [==============================] - 2s 30ms/step - loss: 0.4274 - accuracy: 0.8105\n",
      "basemodel__lr=0.0001__batch_size=64__optimizer=adamw__class_weights=balanced__dropout=0.4 completed.\n",
      "\n",
      "*********************    Session 19/24 ended    ***********************\n",
      "\n",
      "\n",
      "********************    Session 20/24 started    **********************\n",
      "\n",
      "Values are set: Optimizer: adamw | Learning Rate: 0.0001 | Batch Size: 64 | Class Weights: none | Dropout Rate: 0.4\n",
      "Model: \"sequential_138\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_138 (Embedding)   (None, None, 200)         3514600   \n",
      "                                                                 \n",
      " dropout_276 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " bidirectional_371 (Bidirect  (None, None, 256)        336896    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_372 (Bidirect  (None, None, 128)        164352    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_276 (Dense)           (None, None, 64)          8256      \n",
      "                                                                 \n",
      " global_average_pooling1d_13  (None, 64)               0         \n",
      " 8 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dropout_277 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_277 (Dense)           (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,024,234\n",
      "Trainable params: 509,634\n",
      "Non-trainable params: 3,514,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "basemodel__lr=0.0001__batch_size=64__optimizer=adamw__class_weights=none__dropout=0.4 starting...\n",
      "Epoch 1/10\n",
      "70/70 [==============================] - 16s 128ms/step - loss: 0.6839 - accuracy: 0.5670 - val_loss: 0.6498 - val_accuracy: 0.6990\n",
      "Epoch 2/10\n",
      "70/70 [==============================] - 6s 92ms/step - loss: 0.5935 - accuracy: 0.7391 - val_loss: 0.5057 - val_accuracy: 0.7582\n",
      "Epoch 3/10\n",
      "70/70 [==============================] - 6s 93ms/step - loss: 0.5101 - accuracy: 0.7741 - val_loss: 0.4653 - val_accuracy: 0.7901\n",
      "Epoch 4/10\n",
      "70/70 [==============================] - 7s 93ms/step - loss: 0.4821 - accuracy: 0.7935 - val_loss: 0.4538 - val_accuracy: 0.7935\n",
      "Epoch 5/10\n",
      "70/70 [==============================] - 7s 94ms/step - loss: 0.4719 - accuracy: 0.7940 - val_loss: 0.4520 - val_accuracy: 0.7976\n",
      "Epoch 6/10\n",
      "70/70 [==============================] - 7s 94ms/step - loss: 0.4664 - accuracy: 0.7935 - val_loss: 0.4461 - val_accuracy: 0.7996\n",
      "Epoch 7/10\n",
      "70/70 [==============================] - 7s 94ms/step - loss: 0.4589 - accuracy: 0.7989 - val_loss: 0.4453 - val_accuracy: 0.7982\n",
      "Epoch 8/10\n",
      "70/70 [==============================] - 7s 95ms/step - loss: 0.4585 - accuracy: 0.7971 - val_loss: 0.4425 - val_accuracy: 0.7969\n",
      "Epoch 9/10\n",
      "70/70 [==============================] - 7s 95ms/step - loss: 0.4486 - accuracy: 0.8096 - val_loss: 0.4424 - val_accuracy: 0.7989\n",
      "Epoch 10/10\n",
      "70/70 [==============================] - 7s 95ms/step - loss: 0.4468 - accuracy: 0.8107 - val_loss: 0.4439 - val_accuracy: 0.7989\n",
      "23/23 [==============================] - 2s 30ms/step - loss: 0.4236 - accuracy: 0.8152\n",
      "basemodel__lr=0.0001__batch_size=64__optimizer=adamw__class_weights=none__dropout=0.4 completed.\n",
      "\n",
      "*********************    Session 20/24 ended    ***********************\n",
      "\n",
      "\n",
      "********************    Session 21/24 started    **********************\n",
      "\n",
      "Values are set: Optimizer: adamw | Learning Rate: 0.0005 | Batch Size: 64 | Class Weights: balanced | Dropout Rate: 0.1\n",
      "Model: \"sequential_139\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_139 (Embedding)   (None, None, 200)         3514600   \n",
      "                                                                 \n",
      " dropout_278 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " bidirectional_373 (Bidirect  (None, None, 256)        336896    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_374 (Bidirect  (None, None, 128)        164352    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_278 (Dense)           (None, None, 64)          8256      \n",
      "                                                                 \n",
      " global_average_pooling1d_13  (None, 64)               0         \n",
      " 9 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dropout_279 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_279 (Dense)           (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,024,234\n",
      "Trainable params: 509,634\n",
      "Non-trainable params: 3,514,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "basemodel__lr=0.0005__batch_size=64__optimizer=adamw__class_weights=balanced__dropout=0.1 starting...\n",
      "Epoch 1/10\n",
      "70/70 [==============================] - 15s 126ms/step - loss: 0.6081 - accuracy: 0.6623 - val_loss: 0.4835 - val_accuracy: 0.7724\n",
      "Epoch 2/10\n",
      "70/70 [==============================] - 6s 93ms/step - loss: 0.4546 - accuracy: 0.7955 - val_loss: 0.4347 - val_accuracy: 0.7989\n",
      "Epoch 3/10\n",
      "70/70 [==============================] - 6s 92ms/step - loss: 0.4174 - accuracy: 0.8170 - val_loss: 0.4410 - val_accuracy: 0.8023\n",
      "Epoch 4/10\n",
      "70/70 [==============================] - 7s 95ms/step - loss: 0.3996 - accuracy: 0.8263 - val_loss: 0.4459 - val_accuracy: 0.8050\n",
      "Epoch 5/10\n",
      "70/70 [==============================] - 7s 96ms/step - loss: 0.3863 - accuracy: 0.8339 - val_loss: 0.4591 - val_accuracy: 0.8010\n",
      "Epoch 6/10\n",
      "70/70 [==============================] - 7s 98ms/step - loss: 0.3771 - accuracy: 0.8377 - val_loss: 0.4627 - val_accuracy: 0.8050\n",
      "Epoch 7/10\n",
      "70/70 [==============================] - 7s 100ms/step - loss: 0.3676 - accuracy: 0.8491 - val_loss: 0.4573 - val_accuracy: 0.8037\n",
      "Epoch 7: early stopping\n",
      "23/23 [==============================] - 2s 32ms/step - loss: 0.4497 - accuracy: 0.8057\n",
      "basemodel__lr=0.0005__batch_size=64__optimizer=adamw__class_weights=balanced__dropout=0.1 completed.\n",
      "\n",
      "*********************    Session 21/24 ended    ***********************\n",
      "\n",
      "\n",
      "********************    Session 22/24 started    **********************\n",
      "\n",
      "Values are set: Optimizer: adamw | Learning Rate: 0.0005 | Batch Size: 64 | Class Weights: none | Dropout Rate: 0.1\n",
      "Model: \"sequential_140\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_140 (Embedding)   (None, None, 200)         3514600   \n",
      "                                                                 \n",
      " dropout_280 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " bidirectional_375 (Bidirect  (None, None, 256)        336896    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_376 (Bidirect  (None, None, 128)        164352    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_280 (Dense)           (None, None, 64)          8256      \n",
      "                                                                 \n",
      " global_average_pooling1d_14  (None, 64)               0         \n",
      " 0 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dropout_281 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_281 (Dense)           (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,024,234\n",
      "Trainable params: 509,634\n",
      "Non-trainable params: 3,514,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "basemodel__lr=0.0005__batch_size=64__optimizer=adamw__class_weights=none__dropout=0.1 starting...\n",
      "Epoch 1/10\n",
      "70/70 [==============================] - 16s 133ms/step - loss: 0.5976 - accuracy: 0.6897 - val_loss: 0.5181 - val_accuracy: 0.7534\n",
      "Epoch 2/10\n",
      "70/70 [==============================] - 7s 97ms/step - loss: 0.4578 - accuracy: 0.7982 - val_loss: 0.4387 - val_accuracy: 0.7976\n",
      "Epoch 3/10\n",
      "70/70 [==============================] - 7s 99ms/step - loss: 0.4245 - accuracy: 0.8156 - val_loss: 0.4322 - val_accuracy: 0.8030\n",
      "Epoch 4/10\n",
      "70/70 [==============================] - 7s 97ms/step - loss: 0.4076 - accuracy: 0.8297 - val_loss: 0.4339 - val_accuracy: 0.8105\n",
      "Epoch 5/10\n",
      "70/70 [==============================] - 7s 98ms/step - loss: 0.3948 - accuracy: 0.8355 - val_loss: 0.4481 - val_accuracy: 0.8050\n",
      "Epoch 6/10\n",
      "70/70 [==============================] - 7s 99ms/step - loss: 0.3958 - accuracy: 0.8324 - val_loss: 0.4356 - val_accuracy: 0.8091\n",
      "Epoch 7/10\n",
      "70/70 [==============================] - 7s 96ms/step - loss: 0.3780 - accuracy: 0.8408 - val_loss: 0.4361 - val_accuracy: 0.8091\n",
      "Epoch 8/10\n",
      "70/70 [==============================] - 7s 95ms/step - loss: 0.3611 - accuracy: 0.8496 - val_loss: 0.4489 - val_accuracy: 0.8037\n",
      "Epoch 8: early stopping\n",
      "23/23 [==============================] - 2s 30ms/step - loss: 0.4286 - accuracy: 0.8145\n",
      "basemodel__lr=0.0005__batch_size=64__optimizer=adamw__class_weights=none__dropout=0.1 completed.\n",
      "\n",
      "*********************    Session 22/24 ended    ***********************\n",
      "\n",
      "\n",
      "********************    Session 23/24 started    **********************\n",
      "\n",
      "Values are set: Optimizer: adamw | Learning Rate: 0.0005 | Batch Size: 64 | Class Weights: balanced | Dropout Rate: 0.4\n",
      "Model: \"sequential_141\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_141 (Embedding)   (None, None, 200)         3514600   \n",
      "                                                                 \n",
      " dropout_282 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " bidirectional_377 (Bidirect  (None, None, 256)        336896    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_378 (Bidirect  (None, None, 128)        164352    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_282 (Dense)           (None, None, 64)          8256      \n",
      "                                                                 \n",
      " global_average_pooling1d_14  (None, 64)               0         \n",
      " 1 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dropout_283 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_283 (Dense)           (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,024,234\n",
      "Trainable params: 509,634\n",
      "Non-trainable params: 3,514,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "basemodel__lr=0.0005__batch_size=64__optimizer=adamw__class_weights=balanced__dropout=0.4 starting...\n",
      "Epoch 1/10\n",
      "70/70 [==============================] - 14s 125ms/step - loss: 0.6511 - accuracy: 0.5993 - val_loss: 0.5065 - val_accuracy: 0.7500\n",
      "Epoch 2/10\n",
      "70/70 [==============================] - 6s 91ms/step - loss: 0.4835 - accuracy: 0.7844 - val_loss: 0.4464 - val_accuracy: 0.7969\n",
      "Epoch 3/10\n",
      "70/70 [==============================] - 7s 93ms/step - loss: 0.4531 - accuracy: 0.7980 - val_loss: 0.4437 - val_accuracy: 0.7989\n",
      "Epoch 4/10\n",
      "70/70 [==============================] - 6s 92ms/step - loss: 0.4461 - accuracy: 0.8038 - val_loss: 0.4414 - val_accuracy: 0.8050\n",
      "Epoch 5/10\n",
      "70/70 [==============================] - 6s 91ms/step - loss: 0.4333 - accuracy: 0.8094 - val_loss: 0.4300 - val_accuracy: 0.7989\n",
      "Epoch 6/10\n",
      "70/70 [==============================] - 6s 92ms/step - loss: 0.4187 - accuracy: 0.8179 - val_loss: 0.4367 - val_accuracy: 0.7982\n",
      "Epoch 7/10\n",
      "70/70 [==============================] - 6s 92ms/step - loss: 0.4165 - accuracy: 0.8138 - val_loss: 0.4430 - val_accuracy: 0.7914\n",
      "Epoch 8/10\n",
      "70/70 [==============================] - 7s 94ms/step - loss: 0.4035 - accuracy: 0.8203 - val_loss: 0.4498 - val_accuracy: 0.7955\n",
      "Epoch 9/10\n",
      "70/70 [==============================] - 7s 93ms/step - loss: 0.4031 - accuracy: 0.8261 - val_loss: 0.4437 - val_accuracy: 0.7989\n",
      "Epoch 10/10\n",
      "70/70 [==============================] - 7s 94ms/step - loss: 0.3977 - accuracy: 0.8261 - val_loss: 0.4401 - val_accuracy: 0.7996\n",
      "Epoch 10: early stopping\n",
      "23/23 [==============================] - 2s 29ms/step - loss: 0.4208 - accuracy: 0.8152\n",
      "basemodel__lr=0.0005__batch_size=64__optimizer=adamw__class_weights=balanced__dropout=0.4 completed.\n",
      "\n",
      "*********************    Session 23/24 ended    ***********************\n",
      "\n",
      "\n",
      "********************    Session 24/24 started    **********************\n",
      "\n",
      "Values are set: Optimizer: adamw | Learning Rate: 0.0005 | Batch Size: 64 | Class Weights: none | Dropout Rate: 0.4\n",
      "Model: \"sequential_142\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_142 (Embedding)   (None, None, 200)         3514600   \n",
      "                                                                 \n",
      " dropout_284 (Dropout)       (None, None, 200)         0         \n",
      "                                                                 \n",
      " bidirectional_379 (Bidirect  (None, None, 256)        336896    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_380 (Bidirect  (None, None, 128)        164352    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_284 (Dense)           (None, None, 64)          8256      \n",
      "                                                                 \n",
      " global_average_pooling1d_14  (None, 64)               0         \n",
      " 2 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dropout_285 (Dropout)       (None, 64)                0         \n",
      "                                                                 \n",
      " dense_285 (Dense)           (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,024,234\n",
      "Trainable params: 509,634\n",
      "Non-trainable params: 3,514,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "basemodel__lr=0.0005__batch_size=64__optimizer=adamw__class_weights=none__dropout=0.4 starting...\n",
      "Epoch 1/10\n",
      "70/70 [==============================] - 14s 122ms/step - loss: 0.6281 - accuracy: 0.6600 - val_loss: 0.5012 - val_accuracy: 0.7609\n",
      "Epoch 2/10\n",
      "70/70 [==============================] - 6s 88ms/step - loss: 0.4855 - accuracy: 0.7815 - val_loss: 0.4433 - val_accuracy: 0.7962\n",
      "Epoch 3/10\n",
      "70/70 [==============================] - 6s 89ms/step - loss: 0.4472 - accuracy: 0.8067 - val_loss: 0.4365 - val_accuracy: 0.7982\n",
      "Epoch 4/10\n",
      "70/70 [==============================] - 6s 89ms/step - loss: 0.4373 - accuracy: 0.8123 - val_loss: 0.4365 - val_accuracy: 0.8016\n",
      "Epoch 5/10\n",
      "70/70 [==============================] - 6s 90ms/step - loss: 0.4358 - accuracy: 0.8114 - val_loss: 0.4357 - val_accuracy: 0.7969\n",
      "Epoch 6/10\n",
      "70/70 [==============================] - 6s 90ms/step - loss: 0.4273 - accuracy: 0.8154 - val_loss: 0.4408 - val_accuracy: 0.7969\n",
      "Epoch 7/10\n",
      "70/70 [==============================] - 6s 90ms/step - loss: 0.4136 - accuracy: 0.8221 - val_loss: 0.4387 - val_accuracy: 0.7955\n",
      "Epoch 8/10\n",
      "70/70 [==============================] - 6s 91ms/step - loss: 0.4074 - accuracy: 0.8223 - val_loss: 0.4374 - val_accuracy: 0.8016\n",
      "Epoch 9/10\n",
      "70/70 [==============================] - 6s 91ms/step - loss: 0.3988 - accuracy: 0.8270 - val_loss: 0.4428 - val_accuracy: 0.7989\n",
      "Epoch 10/10\n",
      "70/70 [==============================] - 7s 97ms/step - loss: 0.4004 - accuracy: 0.8228 - val_loss: 0.4340 - val_accuracy: 0.8016\n",
      "23/23 [==============================] - 2s 32ms/step - loss: 0.4181 - accuracy: 0.8159\n",
      "basemodel__lr=0.0005__batch_size=64__optimizer=adamw__class_weights=none__dropout=0.4 completed.\n",
      "\n",
      "*********************    Session 24/24 ended    ***********************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "find_best_hparams(dataset=dataset,\n",
    "                  data_pipeline=data_pipeline,\n",
    "                  hyperparameter_manager=hyperparameter_manager,\n",
    "                  epochs=10,\n",
    "                  balanced_class_weights=balanced_class_weights,\n",
    "                  log_directory=log_dir)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensorboard --logdir logs/hyperparameter_tuning_BaseModel --port 5003"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}